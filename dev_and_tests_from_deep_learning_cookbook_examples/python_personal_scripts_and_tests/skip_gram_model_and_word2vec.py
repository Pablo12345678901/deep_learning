import io # Core tools for working with streams
import re # Regex operations
import string
import tqdm # Shows progress bar in Python

import numpy as np

import tensorflow as tf
from tensorflow.keras import layers

# My custom imports
import personal_functions

#####################################################

"""
This code is inspired from the website : https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/word2vec.ipynb 

It shows how to build a 'Continuous skip-gram model' which is able to predict words within a certain range before and after the current word in the same sentence.

While a bag-of-words model predicts a word given the neighboring context, a skip-gram model predicts the context (or neighbors) of a word, given the word itself. The model is trained on skip-grams, which are n-grams that allow tokens to be skipped (see the diagram below for an example). The context of a word can be represented through a set of skip-gram pairs of (target_word, context_word) where context_word appears in the neighboring context of target_word.
"""

# Setup
# Set the random seed
SEED = 42
# tf.data : an API that enables to build complex input pipelines from simple, reusable pieces. 
AUTOTUNE = tf.data.AUTOTUNE # = -1

##############################################

# Vectorize an example sentence
sentence = "The wide road shimmered in the hot sun"
# Below sentence : shows that token can contains the char "'" and does not split 'I'm' into token[0]='I' and token[1]='m'  
#sentence = "I'm the king of my wife's dog."
# Split (with the space char) the sentence into a 'tokens' list.
# And lower all chars - no difference between 'The', 'the' - both consideredas 'the'
tokens = list(sentence.lower().split())
# Show the lenght of the 'tokens' list and its content, index per index
#personal_functions.printing_list_elements(tokens)

# Define a 'vocab' dict and index (int)
vocab, index = {}, 0
# Add a padding token with value 0 because the token of index 0 will be skipped by the tf.keras.preprocessing.sequence.skipgrams function.
vocab['<pad>'] = index
# Increase the index each time a key is added into the dict
# That way, all (unique) keys have a unique index as as unique identifier
index += 1
# Add all tokens into the dict
for token in tokens:
  if token not in vocab:
    vocab[token] = index
    index += 1

# Get the number of keys in the dict
vocab_size = len(vocab)
# Below : shows the dict = its content and its lenght
#print("The dict of tokens has a lenght of : " + str(vocab_size))
#personal_functions.print_dictionary_elements(vocab)

# Create an inverse dict with indexes as keys and tokens as keys values
inverse_vocab = {index: token for token, index in vocab.items()}
inverse_vocab_size = len(inverse_vocab)
# Below : shows the dict = its content and its lenght
#print("The dict of tokens has a lenght of : " + str(inverse_vocab_size))
#personal_functions.print_dictionary_elements(inverse_vocab)

# Vectorize the sentence by replacing all token use with their indexes
example_sequence = [vocab[word] for word in tokens]
# Show the vector created
#print(example_sequence)
# Show the equivalent tokens to the vector from the original sentence
#equivalent_tokens = [ word for word in tokens ]
#print(equivalent_tokens)

##############################################

# Generate skip-grams from one sentence

window_size = 2
# tf.keras.preprocessing.sequence module provides useful functions that simplify data preparation for word2vec.
# tf.keras.preprocessing.sequence.skipgrams : Generates skipgram word pairs.
"""
Function definition : 
tf.keras.preprocessing.sequence.skipgrams(
    sequence,
    vocabulary_size,
    window_size=4,
    negative_samples=1.0,
    shuffle=True,
    categorical=False,
    sampling_table=None,
    seed=None
)
"""
# 'vocabulary_size' : Int, maximum possible word index + 1. Note that index 0 is expected to be a non-word and will be skipped.
# 'negative_samples' is set to 0 as batching negative samples generated by this function requires a bit of code. Another function will be used to perform negative sampling in the next section.
# 'window_size' : number of token considered on each size of the token processed (so the number of token considered is :
# = windows_size + token_processed + windows_size
# = 2*windows_size + 1
# Here : the function returns :
#    couples, labels : where couples are int pairs and labels are either 0 or 1.
window_size = 2
positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(
      example_sequence,
      vocabulary_size=vocab_size,
      window_size=window_size,
      negative_samples=0)
# Show the results returned by the function tf.keras.preprocessing.sequence.skipgrams
#print(positive_skip_grams) # [[3, 2], [2, 3], [4, 5], [1, 7], [5, 6], [3, 5], [5, 1], [3, 1], [1, 3], [1, 2], [1, 5], [4, 1], [1, 6], [7, 1], [6, 1], [5, 4], [6, 5], [3, 4], [6, 7], [2, 1], [1, 4], [7, 6], [5, 3], [4, 3], [4, 2], [2, 4]]
#print(_) # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
#print(len(positive_skip_grams)) # = 26


# To be done


##############################################

# My remarks to be processed before leaving the script
print("\nDEBUG : My remarks to myself")
print("What is 'AUTOTUNE' and why set it to '-1' ?")
print("Understand how to interpret what is returned by the function 'tf.keras.preprocessing.sequence.skipgrams'")
print("If needed : see other detailed explanation of skip gram model here : https://medium.com/@corymaklin/word2vec-skip-gram-904775613b4c")
print("See also : tutorial on how to use word2vec in gensim : https://rare-technologies.com/word2vec-tutorial/")
input("DEBUG : do the above before finishing this script\n") 
